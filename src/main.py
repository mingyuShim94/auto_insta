#!/usr/bin/env python3
"""
Instagram ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° - ë©”ì¸ CLI ì¸í„°í˜ì´ìŠ¤
"""

import sys
import argparse
import time
import os
from typing import Optional, List, Tuple

from .extractor import InstagramTextExtractor
from .selenium_extractor import SeleniumInstagramExtractor
from .utils import (
    format_text_output,
    save_to_file,
    print_error_message,
    print_success_message,
    get_user_confirmation,
)


def parse_arguments() -> argparse.Namespace:
    """ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="Instagram ê²Œì‹œë¬¼ ë§í¬ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.",
        epilog="ì˜ˆì‹œ: python main.py https://www.instagram.com/p/ABC123/",
    )

    parser.add_argument("url", nargs="?", help="Instagram ê²Œì‹œë¬¼ URL")

    parser.add_argument(
        "--metadata",
        "-m",
        action="store_true",
        help="ë©”íƒ€ë°ì´í„° í¬í•¨ (ì‘ì„±ì, ì¢‹ì•„ìš” ìˆ˜, ë‚ ì§œ ë“±)",
    )

    parser.add_argument(
        "--save",
        "-s",
        metavar="FORMAT",
        choices=["txt", "json"],
        help="ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥ (txt ë˜ëŠ” json í˜•ì‹)",
    )

    parser.add_argument(
        "--output", "-o", metavar="FILENAME", help="ì €ì¥í•  íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)"
    )

    parser.add_argument(
        "--quiet", "-q", action="store_true", help="ìµœì†Œí•œì˜ ì¶œë ¥ë§Œ í‘œì‹œ"
    )

    parser.add_argument(
        "--batch-file",
        "-b",
        metavar="FILEPATH",
        help="URL ëª©ë¡ì´ í¬í•¨ëœ íŒŒì¼ ê²½ë¡œ",
    )

    parser.add_argument(
        "--batch",
        metavar="FILEPATH",
        help="ë°°ì¹˜ ëª¨ë“œ ê°„í¸ ì‹¤í–‰ (metadata, json, combined-output ìë™ ì ìš©)",
    )

    parser.add_argument(
        "--combined-output",
        "-c",
        action="store_true",
        help="ëª¨ë“  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ í†µí•© íŒŒì¼ë¡œ ì €ì¥",
    )

    parser.add_argument(
        "--delay",
        "-d",
        type=int,
        default=3,
        help="URL ì²˜ë¦¬ ê°„ ëŒ€ê¸°ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 3ì´ˆ)",
    )

    parser.add_argument(
        "--simple",
        action="store_true",
        help="ê°„ë‹¨í•œ ì¶œë ¥ ëª¨ë“œ (textì™€ urlë§Œ í¬í•¨)",
    )

    parser.add_argument(
        "--with-titles",
        action="store_true",
        help="ì œëª© í¬í•¨ ëª¨ë“œ (ì œëª©::URL í˜•ì‹ íŒŒì¼ ì‚¬ìš©)",
    )

    parser.add_argument(
        "--max-retries",
        type=int,
        default=5,
        help="Instagram API ì—ëŸ¬ ì‹œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)",
    )

    parser.add_argument(
        "--retry-delay",
        type=int,
        default=5,
        help="ì¬ì‹œë„ ì‹œ ì´ˆê¸° ëŒ€ê¸°ì‹œê°„ (ì´ˆ, ê¸°ë³¸ê°’: 5ì´ˆ)",
    )

    parser.add_argument(
        "--use-selenium",
        action="store_true",
        help="Selenium WebDriver ì‚¬ìš© (ê¸°ë³¸ê°’: instaloader)",
    )

    parser.add_argument(
        "--selenium-workers",
        type=int,
        default=5,
        help="Selenium ëª¨ë“œì—ì„œ ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸ê°’: 5)",
    )

    parser.add_argument(
        "--headless",
        action="store_true",
        default=True,
        help="Selenium í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© (ê¸°ë³¸ê°’: True)",
    )

    return parser.parse_args()


def read_urls_from_file(file_path: str) -> List[str]:
    """íŒŒì¼ì—ì„œ URL ëª©ë¡ì„ ì½ì–´ì˜¤ê¸°

    Args:
        file_path (str): URL ëª©ë¡ íŒŒì¼ ê²½ë¡œ

    Returns:
        List[str]: URL ëª©ë¡

    Raises:
        FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        ValueError: íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš°
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        urls = []
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            # ë¹ˆ ì¤„ì´ë‚˜ ì£¼ì„(#ìœ¼ë¡œ ì‹œì‘) ì œì™¸
            if line and not line.startswith('#'):
                if line.startswith('https://') or line.startswith('http://'):
                    urls.append(line)
                else:
                    print(f"âš ï¸ ë¼ì¸ {line_num}: ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹ - {line}")

        if not urls:
            raise ValueError("ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")

        return urls

    except FileNotFoundError:
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    except Exception as e:
        raise ValueError(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")


def read_urls_with_titles_from_file(file_path: str) -> List[Tuple[str, str]]:
    """íŒŒì¼ì—ì„œ ì œëª©ê³¼ URL ëª©ë¡ì„ ì½ì–´ì˜¤ê¸° (ì œëª©::URL í˜•ì‹)

    Args:
        file_path (str): ì œëª©ê³¼ URL ëª©ë¡ íŒŒì¼ ê²½ë¡œ

    Returns:
        List[Tuple[str, str]]: (ì œëª©, URL) íŠœí”Œ ëª©ë¡

    Raises:
        FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        ValueError: íŒŒì¼ í˜•ì‹ì´ ì˜ëª»ëœ ê²½ìš°
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        urls_with_titles = []
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            # ë¹ˆ ì¤„ì´ë‚˜ ì£¼ì„(#ìœ¼ë¡œ ì‹œì‘) ì œì™¸
            if line and not line.startswith('#'):
                if '::' in line:
                    # ì œëª©::URL í˜•ì‹
                    parts = line.split('::', 1)
                    if len(parts) == 2:
                        title = parts[0].strip()
                        url = parts[1].strip()
                        if url.startswith('https://') or url.startswith('http://'):
                            urls_with_titles.append((title, url))
                        else:
                            print(f"âš ï¸ ë¼ì¸ {line_num}: ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹ - {url}")
                    else:
                        print(f"âš ï¸ ë¼ì¸ {line_num}: ì˜ëª»ëœ í˜•ì‹ - {line}")
                elif line.startswith('https://') or line.startswith('http://'):
                    # URLë§Œ ìˆëŠ” ê²½ìš° (í•˜ìœ„ í˜¸í™˜ì„±)
                    urls_with_titles.append(("ë¯¸ì •", line))
                else:
                    print(f"âš ï¸ ë¼ì¸ {line_num}: ìœ íš¨í•˜ì§€ ì•Šì€ í˜•ì‹ - {line}")

        if not urls_with_titles:
            raise ValueError("ìœ íš¨í•œ URLì´ ì—†ìŠµë‹ˆë‹¤.")

        return urls_with_titles

    except FileNotFoundError:
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
    except Exception as e:
        raise ValueError(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")


def save_combined_results(results: List[dict], output_format: str = 'txt', simple_mode: bool = False) -> str:
    """ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ í†µí•© íŒŒì¼ë¡œ ì €ì¥

    Args:
        results (List[dict]): ì²˜ë¦¬ ê²°ê³¼ ëª©ë¡
        output_format (str): ì¶œë ¥ í˜•ì‹ ('txt' ë˜ëŠ” 'json')
        simple_mode (bool): ê°„ë‹¨í•œ ëª¨ë“œ (textì™€ urlë§Œ í¬í•¨)

    Returns:
        str: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    from datetime import datetime
    from .utils import save_to_file
    import json

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if output_format == 'txt':
        # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ í†µí•©
        combined_text = []
        combined_text.append("=" * 80)
        combined_text.append("ğŸ“± Instagram ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼")
        combined_text.append(f"ğŸ“… ì²˜ë¦¬ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}")
        combined_text.append(f"ğŸ“Š ì´ ì²˜ë¦¬ ê±´ìˆ˜: {len(results)}ê°œ")
        combined_text.append("=" * 80)
        combined_text.append("")

        for i, result in enumerate(results, 1):
            combined_text.append(f"[{i:02d}] {result.get('title', 'ë¯¸ì •')}")
            combined_text.append(f"ğŸ”— URL: {result['url']}")
            combined_text.append(f"ğŸ‘¤ ì‘ì„±ì: @{result.get('username', 'Unknown')}")
            combined_text.append(f"â¤ï¸ ì¢‹ì•„ìš”: {result.get('likes', 0):,}ê°œ")
            if result.get('date'):
                date_str = result['date'].strftime('%Yë…„ %mì›” %dì¼ %H:%M')
                combined_text.append(f"ğŸ“… ê²Œì‹œì¼: {date_str}")
            combined_text.append("")
            combined_text.append("ğŸ’¬ ë³¸ë¬¸:")
            combined_text.append(result.get('text', '(í…ìŠ¤íŠ¸ ì—†ìŒ)'))
            combined_text.append("")
            combined_text.append("-" * 60)
            combined_text.append("")

        # íŒŒì¼ ì €ì¥
        output_dir = "outputs"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        filename = f"instagram_batch_combined{'_simple' if simple_mode else ''}_{timestamp}.txt"
        file_path = os.path.join(output_dir, filename)

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(combined_text))

    elif output_format == 'json':
        # JSON í˜•ì‹ìœ¼ë¡œ í†µí•©
        batch_data = {
            'processed_at': datetime.now().isoformat(),
            'total_count': len(results),
            'results': []
        }

        for result in results:
            if simple_mode:
                # ê°„ë‹¨í•œ ëª¨ë“œ: title, text, urlë§Œ í¬í•¨
                json_result = {
                    'title': result.get('title', 'ë¯¸ì •'),
                    'text': result.get('text', ''),
                    'url': result.get('url', '')
                }
            else:
                # ì „ì²´ ëª¨ë“œ: ëª¨ë“  ë°ì´í„° í¬í•¨
                json_result = result.copy()
                if 'date' in json_result and json_result['date']:
                    json_result['date'] = json_result['date'].isoformat()
            batch_data['results'].append(json_result)

        # íŒŒì¼ ì €ì¥
        output_dir = "outputs"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        filename = f"instagram_batch_combined{'_simple' if simple_mode else ''}_{timestamp}.json"
        file_path = os.path.join(output_dir, filename)

        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(batch_data, f, ensure_ascii=False, indent=2)

    return file_path


def process_batch_urls_with_titles(extractor: InstagramTextExtractor, urls_with_titles: List[Tuple[str, str]], args: argparse.Namespace) -> List[dict]:
    """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ URLê³¼ ì œëª© ì²˜ë¦¬

    Args:
        extractor: Instagram í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
        urls_with_titles: ì²˜ë¦¬í•  (ì œëª©, URL) íŠœí”Œ ëª©ë¡
        args: ëª…ë ¹ì¤„ ì¸ìˆ˜

    Returns:
        List[dict]: ì²˜ë¦¬ ì„±ê³µí•œ ê²°ê³¼ ëª©ë¡
    """
    results = []
    failed_urls = []
    total_count = len(urls_with_titles)

    print(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì´ {total_count}ê°œ URL (ì œëª© í¬í•¨)")
    print(f"â±ï¸ URL ê°„ ëŒ€ê¸°ì‹œê°„: {args.delay}ì´ˆ")
    print("=" * 60)

    for i, (title, url) in enumerate(urls_with_titles, 1):
        print(f"\n[{i:02d}/{total_count:02d}] ì²˜ë¦¬ ì¤‘: {title}")
        print(f"    URL: {url}")

        try:
            # ì²« ë²ˆì§¸ URLì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
            if i > 1:
                print(f"â³ {args.delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(args.delay)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì œëª© í¬í•¨, ì¬ì‹œë„ ì„¤ì • ì ìš©)
            post_data = extractor.get_post_text(url, title, args.max_retries, args.retry_delay)
            results.append(post_data)

            # ì„±ê³µ ë©”ì‹œì§€
            username = post_data.get('username', 'Unknown')
            likes = post_data.get('likes', 0)
            print(f"âœ… ì„±ê³µ: @{username} ({likes:,}ê°œ ì¢‹ì•„ìš”)")

            # ê°œë³„ íŒŒì¼ ì €ì¥ (combined-outputì´ ì•„ë‹Œ ê²½ìš°)
            if args.save and not args.combined_output:
                filename = f"batch_{i:02d}_{username}_{int(time.time())}.{args.save}"
                file_path = save_to_file(post_data, filename, args.save)
                print(f"ğŸ’¾ ì €ì¥: {file_path}")

        except Exception as e:
            error_msg = str(e)
            failed_urls.append({'title': title, 'url': url, 'error': error_msg})
            print(f"âŒ ì‹¤íŒ¨: {error_msg}")
            continue

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"âœ… ì„±ê³µ: {len(results)}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")

    # ì‹¤íŒ¨í•œ URL ëª©ë¡ ì €ì¥
    if failed_urls:
        print("\nâŒ ì‹¤íŒ¨í•œ URL ëª©ë¡:")
        for failed in failed_urls:
            print(f"   {failed['title']} - {failed['url']} - {failed['error']}")

        # ì‹¤íŒ¨ ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥
        output_dir = "outputs"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        timestamp = int(time.time())
        failed_file = os.path.join(output_dir, f"failed_urls_with_titles_{timestamp}.txt")
        with open(failed_file, 'w', encoding='utf-8') as f:
            f.write("# ì‹¤íŒ¨í•œ Instagram URL ëª©ë¡ (ì œëª© í¬í•¨)\\n")
            f.write(f"# ì²˜ë¦¬ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n")
            for failed in failed_urls:
                f.write(f"{failed['title']}::{failed['url']}  # ì˜¤ë¥˜: {failed['error']}\\n")
        print(f"ğŸ’¾ ì‹¤íŒ¨ ëª©ë¡ ì €ì¥: {failed_file}")

    return results


def process_batch_urls(extractor: InstagramTextExtractor, urls: List[str], args: argparse.Namespace) -> List[dict]:
    """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ URL ì²˜ë¦¬

    Args:
        extractor: Instagram í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
        urls: ì²˜ë¦¬í•  URL ëª©ë¡
        args: ëª…ë ¹ì¤„ ì¸ìˆ˜

    Returns:
        List[dict]: ì²˜ë¦¬ ì„±ê³µí•œ ê²°ê³¼ ëª©ë¡
    """
    results = []
    failed_urls = []
    total_count = len(urls)

    print(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì´ {total_count}ê°œ URL")
    print(f"â±ï¸ URL ê°„ ëŒ€ê¸°ì‹œê°„: {args.delay}ì´ˆ")
    print("=" * 60)

    for i, url in enumerate(urls, 1):
        print(f"\n[{i:02d}/{total_count:02d}] ì²˜ë¦¬ ì¤‘: {url}")

        try:
            # ì²« ë²ˆì§¸ URLì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
            if i > 1:
                print(f"â³ {args.delay}ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(args.delay)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¬ì‹œë„ ì„¤ì • ì ìš©)
            post_data = extractor.get_post_text(url, "ë¯¸ì •", args.max_retries, args.retry_delay)
            results.append(post_data)

            # ì„±ê³µ ë©”ì‹œì§€
            username = post_data.get('username', 'Unknown')
            likes = post_data.get('likes', 0)
            print(f"âœ… ì„±ê³µ: @{username} ({likes:,}ê°œ ì¢‹ì•„ìš”)")

            # ê°œë³„ íŒŒì¼ ì €ì¥ (combined-outputì´ ì•„ë‹Œ ê²½ìš°)
            if args.save and not args.combined_output:
                filename = f"batch_{i:02d}_{username}_{int(time.time())}.{args.save}"
                file_path = save_to_file(post_data, filename, args.save)
                print(f"ğŸ’¾ ì €ì¥: {file_path}")

        except Exception as e:
            error_msg = str(e)
            failed_urls.append({'url': url, 'error': error_msg})
            print(f"âŒ ì‹¤íŒ¨: {error_msg}")
            continue

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"âœ… ì„±ê³µ: {len(results)}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")

    # ì‹¤íŒ¨í•œ URL ëª©ë¡ ì €ì¥
    if failed_urls:
        print("\nâŒ ì‹¤íŒ¨í•œ URL ëª©ë¡:")
        for failed in failed_urls:
            print(f"   {failed['url']} - {failed['error']}")

        # ì‹¤íŒ¨ ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥
        output_dir = "outputs"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        timestamp = int(time.time())
        failed_file = os.path.join(output_dir, f"failed_urls_{timestamp}.txt")
        with open(failed_file, 'w', encoding='utf-8') as f:
            f.write("# ì‹¤íŒ¨í•œ Instagram URL ëª©ë¡\\n")
            f.write(f"# ì²˜ë¦¬ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n")
            for failed in failed_urls:
                f.write(f"{failed['url']}  # ì˜¤ë¥˜: {failed['error']}\\n")
        print(f"ğŸ’¾ ì‹¤íŒ¨ ëª©ë¡ ì €ì¥: {failed_file}")

    return results


def process_batch_urls_with_selenium(selenium_extractor: SeleniumInstagramExtractor, urls_with_titles: List[Tuple[str, str]], args: argparse.Namespace) -> List[dict]:
    """Seleniumìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬
    
    Args:
        selenium_extractor: Selenium ê¸°ë°˜ Instagram í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°
        urls_with_titles: ì²˜ë¦¬í•  (ì œëª©, URL) íŠœí”Œ ëª©ë¡
        args: ëª…ë ¹ì¤„ ì¸ìˆ˜
        
    Returns:
        List[dict]: ì²˜ë¦¬ ì„±ê³µí•œ ê²°ê³¼ ëª©ë¡
    """
    print(f"ğŸš€ Selenium ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì´ {len(urls_with_titles)}ê°œ URL")
    print(f"ğŸ§µ ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜: {args.selenium_workers}")
    print(f"ğŸ­ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ: {'ON' if args.headless else 'OFF'}")
    print("=" * 60)
    
    # Selenium ë°°ì¹˜ ì¶”ì¶œ ì‹¤í–‰
    selenium_results = selenium_extractor.batch_extract(urls_with_titles)
    
    # ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    results = []
    failed_urls = []
    
    for result in selenium_results:
        if result.success:
            # ì„±ê³µí•œ ê²½ìš° ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            post_data = {
                'title': result.title,
                'text': result.text,
                'username': result.username,
                'url': result.url,
                'likes': 0,  # Seleniumì—ì„œëŠ” ì¢‹ì•„ìš” ìˆ˜ë¥¼ ê°€ì ¸ì˜¤ì§€ ì•ŠìŒ
                'date': None,  # Seleniumì—ì„œëŠ” ë‚ ì§œë¥¼ ê°€ì ¸ì˜¤ì§€ ì•ŠìŒ
                'media_count': 1,
                'is_video': result.text == "ë™ì˜ìƒì½˜í…ì¸ "
            }
            results.append(post_data)
        else:
            # ì‹¤íŒ¨í•œ ê²½ìš°
            failed_urls.append({
                'title': result.title,
                'url': result.url,
                'error': result.error_message
            })
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š Selenium ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
    print(f"âœ… ì„±ê³µ: {len(results)}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {len(failed_urls)}ê°œ")
    
    # ì‹¤íŒ¨í•œ URL ëª©ë¡ ì €ì¥
    if failed_urls:
        print("\nâŒ ì‹¤íŒ¨í•œ URL ëª©ë¡:")
        for failed in failed_urls:
            print(f"   {failed['title']} - {failed['url']} - {failed['error']}")
        
        # ì‹¤íŒ¨ ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥
        output_dir = "outputs"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        
        timestamp = int(time.time())
        failed_file = os.path.join(output_dir, f"selenium_failed_urls_{timestamp}.txt")
        with open(failed_file, 'w', encoding='utf-8') as f:
            f.write("# Selenium ì‹¤íŒ¨í•œ Instagram URL ëª©ë¡\\n")
            f.write(f"# ì²˜ë¦¬ì¼ì‹œ: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n")
            for failed in failed_urls:
                f.write(f"{failed['title']}::{failed['url']}  # ì˜¤ë¥˜: {failed['error']}\\n")
        print(f"ğŸ’¾ ì‹¤íŒ¨ ëª©ë¡ ì €ì¥: {failed_file}")
    
    return results


def get_url_interactively() -> Optional[str]:
    """ëŒ€í™”í˜•ìœ¼ë¡œ URL ì…ë ¥ë°›ê¸°"""
    print("ğŸ¯ Instagram ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°")
    print("-" * 40)
    print("Instagram ê²Œì‹œë¬¼ ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("(ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit' ì…ë ¥)")
    print()

    while True:
        url = input("ğŸ“± Instagram URL: ").strip()

        if url.lower() in ["quit", "exit", "q"]:
            return None

        if url:
            return url

        print("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")


def process_single_url(
    extractor: InstagramTextExtractor, url: str, args: argparse.Namespace
) -> bool:
    """ë‹¨ì¼ URL ì²˜ë¦¬

    Returns:
        bool: ì„±ê³µ ì—¬ë¶€
    """
    try:
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if not args.quiet:
            print("ğŸ“¥ ê²Œì‹œë¬¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

        post_data = extractor.get_post_text(url, "ë¯¸ì •", args.max_retries, args.retry_delay)

        # ì½˜ì†” ì¶œë ¥
        if not args.quiet:
            print(format_text_output(post_data, show_metadata=args.metadata))
        else:
            # quiet ëª¨ë“œì—ì„œëŠ” ë³¸ë¬¸ë§Œ ì¶œë ¥
            print(post_data.get("text", ""))

        # íŒŒì¼ ì €ì¥
        if args.save:
            filename = None
            if args.output:
                filename = f"{args.output}.{args.save}"

            file_path = save_to_file(post_data, filename, args.save)
            if not args.quiet:
                print_success_message(f"ê²°ê³¼ë¥¼ {file_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        return True

    except ValueError as e:
        print_error_message("url_error", str(e))
        return False
    except PermissionError as e:
        print_error_message("permission_error", str(e))
        return False
    except ConnectionError as e:
        print_error_message("network_error", str(e))
        return False
    except Exception as e:
        print_error_message("general_error", f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        return False


def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    extractor = InstagramTextExtractor()

    while True:
        url = get_url_interactively()
        if url is None:
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì²˜ë¦¬
        args = argparse.Namespace(metadata=True, save=None, output=None, quiet=False)

        success = process_single_url(extractor, url, args)

        if success:
            # íŒŒì¼ ì €ì¥ ì—¬ë¶€ í™•ì¸
            if get_user_confirmation("ğŸ“ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
                print("íŒŒì¼ í˜•ì‹ì„ ì„ íƒí•˜ì„¸ìš”:")
                print("1. í…ìŠ¤íŠ¸ íŒŒì¼ (.txt)")
                print("2. JSON íŒŒì¼ (.json)")

                while True:
                    choice = input("ì„ íƒ (1-2): ").strip()
                    if choice == "1":
                        args.save = "txt"
                        break
                    elif choice == "2":
                        args.save = "json"
                        break
                    else:
                        print("1 ë˜ëŠ” 2ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

                # ë‹¤ì‹œ ì²˜ë¦¬ (íŒŒì¼ ì €ì¥ í¬í•¨, ê¸°ë³¸ ì¬ì‹œë„ ì„¤ì •)
                post_data = extractor.get_post_text(url, "ë¯¸ì •", 5, 5)
                file_path = save_to_file(post_data, None, args.save)
                print_success_message(f"ê²°ê³¼ë¥¼ {file_path}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        print("\n" + "=" * 60 + "\n")

        if not get_user_confirmation("ğŸ”„ ë‹¤ë¥¸ ê²Œì‹œë¬¼ì„ ì²˜ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"):
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()

    # ê°„í¸ ë°°ì¹˜ ëª¨ë“œ (--batch)
    if args.batch:
        # ê¸°ë³¸ê°’ ìë™ ì„¤ì •
        args.batch_file = args.batch
        args.metadata = True
        args.save = 'json'
        args.combined_output = True

        print("ğŸš€ ê°„í¸ ë°°ì¹˜ ëª¨ë“œ ì‹¤í–‰")
        print("âœ… ìë™ ì„¤ì •: metadata=True, save=json, combined_output=True")
        print("")

    # ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ëª¨ë“œ
    if args.batch_file:
        try:
            # ì œëª©ê³¼ URL ëª©ë¡ íŒŒì¼ ì½ê¸° (í•­ìƒ ì œëª© í¬í•¨ ëª¨ë“œ ì‚¬ìš©)
            urls_with_titles = read_urls_with_titles_from_file(args.batch_file)
            print(f"ğŸ“‚ íŒŒì¼ì—ì„œ {len(urls_with_titles)}ê°œ URLì„ ì½ì—ˆìŠµë‹ˆë‹¤: {args.batch_file}")

            # Selenium ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            if args.use_selenium:
                # Selenium ì¶”ì¶œê¸° ìƒì„±
                selenium_extractor = SeleniumInstagramExtractor(
                    headless=args.headless,
                    max_workers=args.selenium_workers
                )
                print(f"ğŸ”§ Selenium WebDriver ëª¨ë“œ ì‚¬ìš©")
                
                # Selenium ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                results = process_batch_urls_with_selenium(selenium_extractor, urls_with_titles, args)
            else:
                # ê¸°ì¡´ instaloader ë°©ì‹
                extractor = InstagramTextExtractor()
                print(f"ğŸ”§ Instaloader ëª¨ë“œ ì‚¬ìš©")
                
                # ê¸°ì¡´ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                results = process_batch_urls_with_titles(extractor, urls_with_titles, args)

            # í†µí•© ê²°ê³¼ ì €ì¥
            if args.combined_output and results:
                output_format = args.save or 'txt'
                combined_file = save_combined_results(results, output_format, args.simple)
                print(f"ğŸ“„ í†µí•© ê²°ê³¼ ì €ì¥: {combined_file}")

            # ì„±ê³µë¥ ì— ë”°ë¥¸ ì¢…ë£Œ ì½”ë“œ
            total_urls = len(urls_with_titles)
            success_rate = len(results) / total_urls if total_urls else 0
            exit_code = 0 if success_rate >= 0.5 else 1  # 50% ì´ìƒ ì„±ê³µì‹œ ì •ìƒ ì¢…ë£Œ
            sys.exit(exit_code)

        except (FileNotFoundError, ValueError) as e:
            print(f"âŒ ë°°ì¹˜ íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            sys.exit(1)

    # URLì´ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ëŒ€í™”í˜• ëª¨ë“œ
    elif not args.url:
        interactive_mode()
        return

    # ëª…ë ¹ì¤„ ëª¨ë“œ (ë‹¨ì¼ URL)
    else:
        extractor = InstagramTextExtractor()
        success = process_single_url(extractor, args.url, args)
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)
