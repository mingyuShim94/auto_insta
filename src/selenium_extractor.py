"""
Selenium ê¸°ë°˜ Instagram ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ ëª¨ë“ˆ
"""

import re
import time
import random
from typing import Dict, List, Optional
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import (
    TimeoutException, 
    NoSuchElementException, 
    WebDriverException,
    StaleElementReferenceException
)
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup


@dataclass
class ExtractResult:
    """ì¶”ì¶œ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    url: str
    text: str
    username: str
    success: bool
    error_message: str = ""
    extraction_time: str = ""


class SeleniumInstagramExtractor:
    """Selenium ê¸°ë°˜ Instagram ê²Œì‹œë¬¼ í…ìŠ¤íŠ¸ ì¶”ì¶œê¸°"""
    
    def __init__(self, headless: bool = True, max_workers: int = 5):
        """
        ì´ˆê¸°í™”
        
        Args:
            headless (bool): í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            max_workers (int): ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜
        """
        self.headless = headless
        self.max_workers = max_workers
        self.user_agents = [
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        
    def _create_driver(self) -> webdriver.Chrome:
        """Chrome WebDriver ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        try:
            options = Options()
            
            if self.headless:
                options.add_argument("--headless")
            
            # ë°˜ê°ì§€ ë° ì„±ëŠ¥ ìµœì í™” ì˜µì…˜
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            options.add_argument("--disable-extensions")
            options.add_argument("--disable-plugins")
            options.add_argument("--disable-images")
            options.add_argument("--disable-javascript")  # JavaScript ë¹„í™œì„±í™”ë¡œ ë¹ ë¥¸ ë¡œë”©
            
            # ëœë¤ User-Agent ì„¤ì •
            user_agent = random.choice(self.user_agents)
            options.add_argument(f"--user-agent={user_agent}")
            
            # ChromeDriver ì„œë¹„ìŠ¤ ì„¤ì •
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=options)
            
            # ìŠ¤í¬ë¦½íŠ¸ ê°ì§€ ë°©ì§€
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # ì•”ë¬µì  ëŒ€ê¸° ì„¤ì •
            driver.implicitly_wait(10)
            
            return driver
            
        except Exception as e:
            raise RuntimeError(f"ChromeDriver ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def validate_url(self, url: str) -> bool:
        """Instagram URL ìœ íš¨ì„± ê²€ì¦"""
        try:
            parsed_url = urlparse(url)
            
            if parsed_url.netloc not in ["www.instagram.com", "instagram.com"]:
                return False
                
            path_pattern = r"^/(p|reel|tv)/([A-Za-z0-9_-]+)/?"
            if not re.match(path_pattern, parsed_url.path):
                return False
                
            return True
        except Exception:
            return False
    
    def _extract_text_from_page(self, driver: webdriver.Chrome, url: str) -> Dict[str, str]:
        """í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ìµœëŒ€ 15ì´ˆ)
            wait = WebDriverWait(driver, 15)
            
            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì…€ë ‰í„° ì‹œë„
            caption_selectors = [
                "meta[property='og:description']",
                "meta[name='description']",
                "[data-testid='post-caption']", 
                "article h1",
                ".x1lliihq.x1plvlek.xryxfnj.x1n2onr6.x193iq5w.xeuugli.x1fj9vlw.x13faqbe.x1vvkbs.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.x1i0vuye.xvs91rp.xo1l8bm.x5n08af.x10wh9bi.x1wdrske.x8viiok.x18hxmgj",
                "span._aacl._aaco._aacu._aacx._aad7._aade",
                "div._a9zs"
            ]
            
            username = "ì•Œ ìˆ˜ ì—†ìŒ"
            caption_text = ""
            
            # ì‚¬ìš©ìëª… ì¶”ì¶œ
            try:
                username_selectors = [
                    "a[href*='/'][role='link'] span",
                    "header a span",
                    ".x1lliihq.x1plvlek.xryxfnj.x1n2onr6.x193iq5w.xeuugli.x1fj9vlw.x13faqbe.x1vvkbs.xtrsf7v.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.x1i0vuye.xvs91rp.x1rod4b6.xo1l8bm.x10wh9bi.x1wdrske.x8viiok.x18hxmgj"
                ]
                
                for selector in username_selectors:
                    try:
                        elements = driver.find_elements(By.CSS_SELECTOR, selector)
                        for element in elements:
                            text = element.text.strip()
                            if text and not text.startswith('@') and len(text) > 0:
                                username = text
                                break
                        if username != "ì•Œ ìˆ˜ ì—†ìŒ":
                            break
                    except:
                        continue
                        
            except Exception:
                pass
            
            # ìº¡ì…˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for selector in caption_selectors:
                try:
                    if selector.startswith("meta"):
                        # ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
                        element = driver.find_element(By.CSS_SELECTOR, selector)
                        content = element.get_attribute("content")
                        if content and len(content.strip()) > 10:
                            caption_text = content.strip()
                            break
                    else:
                        # ì¼ë°˜ ìš”ì†Œì—ì„œ ì¶”ì¶œ
                        element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))
                        text = element.text.strip()
                        if text and len(text) > 10:
                            caption_text = text
                            break
                except:
                    continue
            
            # BeautifulSoupë¡œ ì¶”ê°€ íŒŒì‹± ì‹œë„
            if not caption_text:
                try:
                    soup = BeautifulSoup(driver.page_source, 'html.parser')
                    
                    # og:description ë©”íƒ€ íƒœê·¸
                    meta_desc = soup.find('meta', property='og:description')
                    if meta_desc and meta_desc.get('content'):
                        caption_text = meta_desc['content'].strip()
                    
                    # ì¼ë°˜ description ë©”íƒ€ íƒœê·¸
                    if not caption_text:
                        meta_desc = soup.find('meta', attrs={'name': 'description'})
                        if meta_desc and meta_desc.get('content'):
                            caption_text = meta_desc['content'].strip()
                            
                except Exception:
                    pass
            
            # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹„ë””ì˜¤ ì»¨í…ì¸ ë¡œ ì²˜ë¦¬
            if not caption_text or len(caption_text.strip()) < 5:
                caption_text = "ë™ì˜ìƒì½˜í…ì¸ "
            
            return {
                "username": username,
                "text": self._clean_text(caption_text)
            }
            
        except TimeoutException:
            return {"username": "ì•Œ ìˆ˜ ì—†ìŒ", "text": "ë™ì˜ìƒì½˜í…ì¸ "}
        except Exception as e:
            raise RuntimeError(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
    
    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return "ë™ì˜ìƒì½˜í…ì¸ "
            
        # ë¶ˆí•„ìš”í•œ ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì •ì œ
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # Instagram ê´€ë ¨ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
        unwanted_patterns = [
            r'Instagramì—ì„œ ì´ ê²Œì‹œë¬¼ ë³´ê¸°',
            r'shared a post on Instagram',
            r'íŒ”ë¡œìš°',
            r'likes',
            r'followers',
            r'following',
            r'Follow',
            r'Posted by'
        ]
        
        for pattern in unwanted_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        text = text.strip()
        
        if len(text) < 5:
            return "ë™ì˜ìƒì½˜í…ì¸ "
            
        return text
    
    def extract_single_url(self, url: str, title: str = "ë¯¸ì •") -> ExtractResult:
        """ë‹¨ì¼ URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        driver = None
        try:
            if not self.validate_url(url):
                return ExtractResult(
                    title=title,
                    url=url,
                    text="",
                    username="",
                    success=False,
                    error_message="ìœ íš¨í•˜ì§€ ì•Šì€ Instagram URL"
                )
            
            driver = self._create_driver()
            
            # ëœë¤ ëŒ€ê¸° (1-3ì´ˆ)
            time.sleep(random.uniform(1, 3))
            
            result = self._extract_text_from_page(driver, url)
            
            return ExtractResult(
                title=title,
                url=url,
                text=result["text"],
                username=result["username"],
                success=True,
                extraction_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            )
            
        except Exception as e:
            return ExtractResult(
                title=title,
                url=url,
                text="",
                username="",
                success=False,
                error_message=str(e)
            )
        finally:
            if driver:
                try:
                    driver.quit()
                except:
                    pass
    
    def batch_extract(self, url_data: List[tuple]) -> List[ExtractResult]:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ URLì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        
        Args:
            url_data: (title, url) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[ExtractResult]: ì¶”ì¶œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        print(f"ğŸš€ {len(url_data)}ê°œ URL ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ìµœëŒ€ {self.max_workers}ê°œ ìŠ¤ë ˆë“œ)")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  ì‘ì—… ì œì¶œ
            future_to_data = {
                executor.submit(self.extract_single_url, url, title): (title, url)
                for title, url in url_data
            }
            
            # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
            for i, future in enumerate(as_completed(future_to_data), 1):
                title, url = future_to_data[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    status = "âœ… ì„±ê³µ" if result.success else f"âŒ ì‹¤íŒ¨: {result.error_message}"
                    print(f"  [{i}/{len(url_data)}] {title[:30]}... - {status}")
                    
                except Exception as e:
                    # ì˜ˆì™¸ ë°œìƒ ì‹œ ì‹¤íŒ¨ ê²°ê³¼ ìƒì„±
                    result = ExtractResult(
                        title=title,
                        url=url,
                        text="",
                        username="",
                        success=False,
                        error_message=f"ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"
                    )
                    results.append(result)
                    print(f"  [{i}/{len(url_data)}] {title[:30]}... - âŒ ì˜ˆì™¸: {str(e)}")
        
        # ì„±ê³µ/ì‹¤íŒ¨ í†µê³„ ì¶œë ¥
        successful = sum(1 for r in results if r.success)
        failed = len(results) - successful
        print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {successful}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")
        
        return results